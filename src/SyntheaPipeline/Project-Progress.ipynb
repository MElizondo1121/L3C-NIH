{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb7a5a3",
   "metadata": {},
   "source": [
    "# Project Assignments\n",
    "\n",
    "* [Project Proposal](https://canvas.txstate.edu/courses/1993336/quizzes/6830611) \n",
    "* [Project Progress](https://canvas.txstate.edu/courses/1993336/assignments/27480554) \n",
    "* [Project Submission](https://canvas.txstate.edu/courses/1993336/assignments/27480566) \n",
    "\n",
    "\n",
    "# PROJECT FORMAT\n",
    "\n",
    "#  Project Synthea\n",
    "\n",
    "**Bridget Bangert & Aaron Parish** \n",
    "\n",
    "* Link to the Project Repository: https://git.txstate.edu/a-p789/ML4347-project-synthea\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "* Our goal is to use the Synthea dataset with 10k patients to predict whether or not a patient will die from COVID-19 based on their medical history. We will most of the datasets that give descriptions of a patients characteristics and medical history such as obervations, conditions, age, gender, etc.\n",
    "\n",
    "* We are using a basic supervised machine learning model for classification if a patient dies from COVID-19 related reasons, so either logistic regression, SVM, gradient boosting/bagging.\n",
    "\n",
    "* The data comes from Synthea, specifically the 10k patient COVID-19 dataset. The link to download the data can be found here:\n",
    "\n",
    "https://synthea.mitre.org/downloads\n",
    "\n",
    "* We plan to use a classifying model, possibly Logistic Regression for the base model and then a more complex one if the performance is not where we want it to be. I have been cleaning the data based on features that we do not wish to use such as payment information, insurance, location, etc. to minimize the possibility of overfitting. For cross-validation, we could potientially implement a grid search to fine-tune the parameters used in the model we choose or by simply plotting the precicion/recall curve.\n",
    "\n",
    "* We hope to build a predictive model to predict whether a patient, given their characteristics and medical history, will die from COVID-19 related reasons. I hope to learn how to preprocess data in a productive manner and the process of working with unclean, noisy data to improve my intuition on building predictive models.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "* Our data consists of 16 datasets displaying information of 10k synthetic patients. We only plan on using 7 of these datasets (conditions, immunizations, observations, patients, procedures, and providers), depending on these datasets yield useful information for our task. Each of them have different shapes and features, but we really only are taking 1-3 features from each one to merge them into one large dataframe of about 10k rows.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "* The EDA graphs I have used so bar:\n",
    "    * Boxplot: to visualize the disribution between a COVID-19 death vs. non-COVID-19 deaths based on age.\n",
    "    * Bar charts: for various aspects (mostly to show frequencies in our data), they are attached below.\n",
    "\n",
    "![My Image](Images/boxplot.png)\n",
    "\n",
    "![My Image](Images/conditions_coviddeath.png)\n",
    "\n",
    "![My Image](Images/covid_death.png)\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "* We are reduing the dimensions of our dataframes by feature selection to relevant information based on our task. As mentioned above, we are dropping many features such as payment information, insurance, and medications. Our observations dataframe has a substancial amount of information that we may not need, so we may have to greatly reduce the dimensions. There are not many continuous features in our datasets other than observations and age. We most likely will not scale the age feature since the range is only ~100 years. For observations, we may have to use scaling since our measurements very greatly.\n",
    "\n",
    "## Machine Learning Approaches\n",
    "\n",
    "* We are in the process of feature selection and merging the dataframes into one large dataframe. We then will use a supervised learning classification model then use the model's scores and a confusion matrix to evaluate its performance. If it is not to our satisfaction, then we shall use a different model or feed more data into our large dataframe. We plan on using either Logistic Regression, Gradient boosting/bagging/SVM. If the performance is not sufficient, we shall use an alternative approach to this method.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
