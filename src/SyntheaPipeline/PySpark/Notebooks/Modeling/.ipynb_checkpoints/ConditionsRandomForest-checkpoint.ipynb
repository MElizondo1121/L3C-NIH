{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba915f29",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Random Forest learning algorithm for classification. It supports both binary and multiclass labels, as well as both continuous and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2544ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/03 13:42:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/03 13:42:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ConditionFeatures').getOrCreate()\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorSlicer, VectorAssembler, ChiSqSelector, VectorIndexer, UnivariateFeatureSelector, VarianceThresholdSelector\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da662c9",
   "metadata": {},
   "source": [
    "import pyspark as pyspark\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '16g'),('spark.driver.memory','16g')])\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce0e3",
   "metadata": {},
   "source": [
    "### Reading and Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4824e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",True).csv('../../synthea-sample-data/data/10k_synthea_covid19_csv/conditions.csv').select('PATIENT','Code', 'Description')\n",
    "deathDf = spark.read.option(\"header\",True).csv('../../synthea-sample-data/data/10k_synthea_covid19_csv/patients.csv').select('Id', 'DEATHDATE')\n",
    "deadSet = df.join(deathDf, (df.PATIENT == deathDf.Id)).na.drop().drop('Id', 'Code')\n",
    "labels = spark.read.option(\"header\",True).csv('../FeatureSelection/dfCovid_DeceasedCovid.csv').select('PATIENT', 'covid-19', 'deceased & covid-19')\n",
    "\n",
    "merged = df.join(deathDf, (df.PATIENT == deathDf.Id), 'left').drop( 'Id')\n",
    "\n",
    "merged = merged.withColumn('deceased', when(col('DEATHDATE').isNotNull(), 1)).na.fill(0)\n",
    "merged = merged.join(labels, ('PATIENT'), 'left').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33c0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedDf = merged.groupBy(\"PATIENT\", 'Code').pivot(\"Code\").agg(count(\"Code\").alias(\"count\")).na.fill(0)\n",
    "merged =merged.select('PATIENT', 'deceased', 'covid-19', 'deceased & covid-19')\n",
    "finalDf = groupedDf.join(merged, ['PATIENT'], 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa0cc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PATIENT: string (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- 10509002: long (nullable = true)\n",
      " |-- 109838007: long (nullable = true)\n",
      " |-- 110030002: long (nullable = true)\n",
      " |-- 124171000119105: long (nullable = true)\n",
      " |-- 126906006: long (nullable = true)\n",
      " |-- 127013003: long (nullable = true)\n",
      " |-- 127295002: long (nullable = true)\n",
      " |-- 128613002: long (nullable = true)\n",
      " |-- 132281000119108: long (nullable = true)\n",
      " |-- 1501000119109: long (nullable = true)\n",
      " |-- 1551000119108: long (nullable = true)\n",
      " |-- 156073000: long (nullable = true)\n",
      " |-- 157141000119108: long (nullable = true)\n",
      " |-- 15777000: long (nullable = true)\n",
      " |-- 16114001: long (nullable = true)\n",
      " |-- 161622006: long (nullable = true)\n",
      " |-- 162573006: long (nullable = true)\n",
      " |-- 162864005: long (nullable = true)\n",
      " |-- 1734006: long (nullable = true)\n",
      " |-- 185086009: long (nullable = true)\n",
      " |-- 190905008: long (nullable = true)\n",
      " |-- 19169002: long (nullable = true)\n",
      " |-- 192127007: long (nullable = true)\n",
      " |-- 195662009: long (nullable = true)\n",
      " |-- 195967001: long (nullable = true)\n",
      " |-- 196416002: long (nullable = true)\n",
      " |-- 197927001: long (nullable = true)\n",
      " |-- 198992004: long (nullable = true)\n",
      " |-- 200936003: long (nullable = true)\n",
      " |-- 201834006: long (nullable = true)\n",
      " |-- 22298006: long (nullable = true)\n",
      " |-- 225444004: long (nullable = true)\n",
      " |-- 230265002: long (nullable = true)\n",
      " |-- 230690007: long (nullable = true)\n",
      " |-- 232353008: long (nullable = true)\n",
      " |-- 233604007: long (nullable = true)\n",
      " |-- 233678006: long (nullable = true)\n",
      " |-- 234466008: long (nullable = true)\n",
      " |-- 235919008: long (nullable = true)\n",
      " |-- 236077008: long (nullable = true)\n",
      " |-- 237602007: long (nullable = true)\n",
      " |-- 239720000: long (nullable = true)\n",
      " |-- 239872002: long (nullable = true)\n",
      " |-- 239873007: long (nullable = true)\n",
      " |-- 24079001: long (nullable = true)\n",
      " |-- 241929008: long (nullable = true)\n",
      " |-- 246677007: long (nullable = true)\n",
      " |-- 248595008: long (nullable = true)\n",
      " |-- 249497008: long (nullable = true)\n",
      " |-- 25064002: long (nullable = true)\n",
      " |-- 254632001: long (nullable = true)\n",
      " |-- 254637007: long (nullable = true)\n",
      " |-- 254837009: long (nullable = true)\n",
      " |-- 262574004: long (nullable = true)\n",
      " |-- 263102004: long (nullable = true)\n",
      " |-- 267036007: long (nullable = true)\n",
      " |-- 267060006: long (nullable = true)\n",
      " |-- 267102003: long (nullable = true)\n",
      " |-- 26929004: long (nullable = true)\n",
      " |-- 271737000: long (nullable = true)\n",
      " |-- 271825005: long (nullable = true)\n",
      " |-- 275272006: long (nullable = true)\n",
      " |-- 283371005: long (nullable = true)\n",
      " |-- 283385000: long (nullable = true)\n",
      " |-- 284549007: long (nullable = true)\n",
      " |-- 284551006: long (nullable = true)\n",
      " |-- 301011002: long (nullable = true)\n",
      " |-- 302870006: long (nullable = true)\n",
      " |-- 307731004: long (nullable = true)\n",
      " |-- 30832001: long (nullable = true)\n",
      " |-- 314994000: long (nullable = true)\n",
      " |-- 33737001: long (nullable = true)\n",
      " |-- 359817006: long (nullable = true)\n",
      " |-- 35999006: long (nullable = true)\n",
      " |-- 363406005: long (nullable = true)\n",
      " |-- 367498001: long (nullable = true)\n",
      " |-- 368581000119106: long (nullable = true)\n",
      " |-- 36955009: long (nullable = true)\n",
      " |-- 36971009: long (nullable = true)\n",
      " |-- 370143000: long (nullable = true)\n",
      " |-- 370247008: long (nullable = true)\n",
      " |-- 386661006: long (nullable = true)\n",
      " |-- 38822007: long (nullable = true)\n",
      " |-- 389087006: long (nullable = true)\n",
      " |-- 398254007: long (nullable = true)\n",
      " |-- 39848009: long (nullable = true)\n",
      " |-- 399211009: long (nullable = true)\n",
      " |-- 40055000: long (nullable = true)\n",
      " |-- 40095003: long (nullable = true)\n",
      " |-- 40275004: long (nullable = true)\n",
      " |-- 403190006: long (nullable = true)\n",
      " |-- 403191005: long (nullable = true)\n",
      " |-- 403192003: long (nullable = true)\n",
      " |-- 408512008: long (nullable = true)\n",
      " |-- 410429000: long (nullable = true)\n",
      " |-- 422034002: long (nullable = true)\n",
      " |-- 422587007: long (nullable = true)\n",
      " |-- 424132000: long (nullable = true)\n",
      " |-- 427089005: long (nullable = true)\n",
      " |-- 428251008: long (nullable = true)\n",
      " |-- 429007001: long (nullable = true)\n",
      " |-- 429280009: long (nullable = true)\n",
      " |-- 431855005: long (nullable = true)\n",
      " |-- 431856006: long (nullable = true)\n",
      " |-- 433144002: long (nullable = true)\n",
      " |-- 43724002: long (nullable = true)\n",
      " |-- 43878008: long (nullable = true)\n",
      " |-- 44054006: long (nullable = true)\n",
      " |-- 443165006: long (nullable = true)\n",
      " |-- 444448004: long (nullable = true)\n",
      " |-- 444470001: long (nullable = true)\n",
      " |-- 44465007: long (nullable = true)\n",
      " |-- 444814009: long (nullable = true)\n",
      " |-- 446096008: long (nullable = true)\n",
      " |-- 449868002: long (nullable = true)\n",
      " |-- 47200007: long (nullable = true)\n",
      " |-- 47505003: long (nullable = true)\n",
      " |-- 47693006: long (nullable = true)\n",
      " |-- 48333001: long (nullable = true)\n",
      " |-- 49436004: long (nullable = true)\n",
      " |-- 49727002: long (nullable = true)\n",
      " |-- 53741008: long (nullable = true)\n",
      " |-- 55680006: long (nullable = true)\n",
      " |-- 55822004: long (nullable = true)\n",
      " |-- 56018004: long (nullable = true)\n",
      " |-- 5602001: long (nullable = true)\n",
      " |-- 57676002: long (nullable = true)\n",
      " |-- 58150001: long (nullable = true)\n",
      " |-- 59621000: long (nullable = true)\n",
      " |-- 6072007: long (nullable = true)\n",
      " |-- 60951000119105: long (nullable = true)\n",
      " |-- 62106007: long (nullable = true)\n",
      " |-- 62564004: long (nullable = true)\n",
      " |-- 64859006: long (nullable = true)\n",
      " |-- 65275009: long (nullable = true)\n",
      " |-- 65363002: long (nullable = true)\n",
      " |-- 65710008: long (nullable = true)\n",
      " |-- 65966004: long (nullable = true)\n",
      " |-- 66857006: long (nullable = true)\n",
      " |-- 67782005: long (nullable = true)\n",
      " |-- 67811000119102: long (nullable = true)\n",
      " |-- 68235000: long (nullable = true)\n",
      " |-- 68496003: long (nullable = true)\n",
      " |-- 68962001: long (nullable = true)\n",
      " |-- 698423002: long (nullable = true)\n",
      " |-- 698754002: long (nullable = true)\n",
      " |-- 69896004: long (nullable = true)\n",
      " |-- 703151001: long (nullable = true)\n",
      " |-- 706870000: long (nullable = true)\n",
      " |-- 70704007: long (nullable = true)\n",
      " |-- 707577004: long (nullable = true)\n",
      " |-- 713197008: long (nullable = true)\n",
      " |-- 7200002: long (nullable = true)\n",
      " |-- 72892002: long (nullable = true)\n",
      " |-- 74400008: long (nullable = true)\n",
      " |-- 75498004: long (nullable = true)\n",
      " |-- 76571007: long (nullable = true)\n",
      " |-- 770349000: long (nullable = true)\n",
      " |-- 79586000: long (nullable = true)\n",
      " |-- 80394007: long (nullable = true)\n",
      " |-- 82423001: long (nullable = true)\n",
      " |-- 83664006: long (nullable = true)\n",
      " |-- 840539006: long (nullable = true)\n",
      " |-- 840544004: long (nullable = true)\n",
      " |-- 84114007: long (nullable = true)\n",
      " |-- 84229001: long (nullable = true)\n",
      " |-- 84757009: long (nullable = true)\n",
      " |-- 86175003: long (nullable = true)\n",
      " |-- 87433001: long (nullable = true)\n",
      " |-- 87628006: long (nullable = true)\n",
      " |-- 88805009: long (nullable = true)\n",
      " |-- 90560007: long (nullable = true)\n",
      " |-- 90781000119102: long (nullable = true)\n",
      " |-- 92691004: long (nullable = true)\n",
      " |-- 93761005: long (nullable = true)\n",
      " |-- 94260004: long (nullable = true)\n",
      " |-- 95417003: long (nullable = true)\n",
      " |-- 97331000119101: long (nullable = true)\n",
      " |-- deceased: integer (nullable = true)\n",
      " |-- covid-19: integer (nullable = true)\n",
      " |-- deceased & covid-19: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = list(set(finalDf.columns) - {'PATIENT', 'deceased', 'Code', 'Description', 'covid-19', 'deceased & covid-19'})\n",
    "assembler = VectorAssembler().setInputCols(cols).setOutputCol('features')\n",
    "finalDf = finalDf.withColumn(\"covid-19\", finalDf[\"covid-19\"].cast(IntegerType())).withColumn(\"deceased & covid-19\", finalDf[\"deceased & covid-19\"].cast(IntegerType()))\n",
    "df = assembler.transform(finalDf)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eebffed",
   "metadata": {},
   "source": [
    "### Chi-Squared Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab363a3",
   "metadata": {},
   "source": [
    "Deceased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993fc292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"deceased\")\n",
    "chiResult = selector.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c1b68c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/03 14:03:50 WARN MemoryStore: Not enough space to cache rdd_562_6 in memory! (computed 65.2 MiB so far)\n",
      "23/04/03 14:03:50 WARN MemoryStore: Not enough space to cache rdd_562_9 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:03:50 WARN MemoryStore: Not enough space to cache rdd_562_3 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:03:50 WARN MemoryStore: Not enough space to cache rdd_562_10 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 14:03:50 WARN BlockManager: Persisting block rdd_562_6 to disk instead.\n",
      "23/04/03 14:03:50 WARN BlockManager: Persisting block rdd_562_9 to disk instead.\n",
      "23/04/03 14:03:50 WARN BlockManager: Persisting block rdd_562_10 to disk instead.\n",
      "23/04/03 14:03:50 WARN BlockManager: Persisting block rdd_562_3 to disk instead.\n",
      "23/04/03 14:03:52 WARN MemoryStore: Not enough space to cache rdd_562_0 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:03:52 WARN BlockManager: Persisting block rdd_562_0 to disk instead.\n",
      "23/04/03 14:03:52 WARN MemoryStore: Not enough space to cache rdd_562_4 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:03:52 WARN BlockManager: Persisting block rdd_562_4 to disk instead.\n",
      "23/04/03 14:03:52 WARN MemoryStore: Not enough space to cache rdd_562_5 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:03:52 WARN BlockManager: Persisting block rdd_562_5 to disk instead.\n",
      "23/04/03 14:03:55 WARN MemoryStore: Not enough space to cache rdd_562_7 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:03:55 WARN BlockManager: Persisting block rdd_562_7 to disk instead.\n",
      "23/04/03 14:03:56 WARN MemoryStore: Not enough space to cache rdd_562_2 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:03:56 WARN BlockManager: Persisting block rdd_562_2 to disk instead.\n",
      "23/04/03 14:03:57 WARN MemoryStore: Not enough space to cache rdd_562_6 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 14:03:57 WARN MemoryStore: Not enough space to cache rdd_562_8 in memory! (computed 65.2 MiB so far)\n",
      "23/04/03 14:03:57 WARN BlockManager: Persisting block rdd_562_8 to disk instead.\n",
      "23/04/03 14:03:58 WARN MemoryStore: Not enough space to cache rdd_562_1 in memory! (computed 97.9 MiB so far)\n",
      "23/04/03 14:03:58 WARN BlockManager: Persisting block rdd_562_1 to disk instead.\n",
      "23/04/03 14:03:58 WARN MemoryStore: Not enough space to cache rdd_562_0 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 14:03:58 WARN MemoryStore: Not enough space to cache rdd_562_3 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 14:03:58 WARN MemoryStore: Not enough space to cache rdd_562_10 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:03:59 WARN MemoryStore: Not enough space to cache rdd_562_9 in memory! (computed 97.9 MiB so far)\n",
      "23/04/03 14:03:59 WARN MemoryStore: Not enough space to cache rdd_562_4 in memory! (computed 97.9 MiB so far)\n",
      "23/04/03 14:03:59 WARN MemoryStore: Not enough space to cache rdd_562_2 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:03:59 WARN MemoryStore: Not enough space to cache rdd_562_1 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 14:03:59 WARN MemoryStore: Not enough space to cache rdd_562_8 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:00 WARN MemoryStore: Not enough space to cache rdd_562_7 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:04:00 WARN MemoryStore: Not enough space to cache rdd_562_11 in memory! (computed 147.2 MiB so far)\n",
      "23/04/03 14:04:00 WARN BlockManager: Persisting block rdd_562_11 to disk instead.\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_2 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_8 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_7 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_1 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_6 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_4 in memory! (computed 3.5 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_0 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_10 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_3 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_9 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_9 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_1 in memory! (computed 3.5 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_7 in memory! (computed 3.5 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_3 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_10 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_8 in memory! (computed 2.3 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_0 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_6 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_4 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 14:04:02 WARN MemoryStore: Not enough space to cache rdd_562_2 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_9 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_8 in memory! (computed 2.3 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_7 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_1 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_3 in memory! (computed 3.5 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_6 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_0 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_4 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_10 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 14:04:03 WARN MemoryStore: Not enough space to cache rdd_562_2 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_2 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_8 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_10 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_4 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_0 in memory! (computed 3.5 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_3 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_1 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_9 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_7 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 14:04:04 WARN MemoryStore: Not enough space to cache rdd_562_6 in memory! (computed 18.4 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=5, labelCol=\"deceased\", seed=42,leafCol=\"leafId\")\n",
    "(train, test) = chiResult.randomSplit([0.8, 0.2])\n",
    "model = rf.fit(train).setFeaturesCol('selectedFeatures')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312897d0",
   "metadata": {},
   "source": [
    "Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01251ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = assembler.transform(finalDf)\n",
    "df = df.na.drop()\n",
    "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"covid-19\")\n",
    "chiResult = selector.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a6c66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/03 13:59:44 WARN MemoryStore: Not enough space to cache rdd_225_8 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 13:59:44 WARN MemoryStore: Not enough space to cache rdd_225_6 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 13:59:44 WARN MemoryStore: Not enough space to cache rdd_225_10 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 13:59:44 WARN MemoryStore: Not enough space to cache rdd_225_9 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 13:59:44 WARN MemoryStore: Not enough space to cache rdd_225_2 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:45 WARN BlockManager: Persisting block rdd_225_2 to disk instead.\n",
      "23/04/03 13:59:45 WARN BlockManager: Persisting block rdd_225_8 to disk instead.\n",
      "23/04/03 13:59:45 WARN BlockManager: Persisting block rdd_225_6 to disk instead.\n",
      "23/04/03 13:59:45 WARN BlockManager: Persisting block rdd_225_10 to disk instead.\n",
      "23/04/03 13:59:45 WARN BlockManager: Persisting block rdd_225_9 to disk instead.\n",
      "23/04/03 13:59:47 WARN MemoryStore: Not enough space to cache rdd_225_0 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 13:59:47 WARN BlockManager: Persisting block rdd_225_0 to disk instead.\n",
      "23/04/03 13:59:47 WARN MemoryStore: Not enough space to cache rdd_225_7 in memory! (computed 3.5 MiB so far)\n",
      "23/04/03 13:59:47 WARN BlockManager: Persisting block rdd_225_7 to disk instead.\n",
      "23/04/03 13:59:48 WARN MemoryStore: Not enough space to cache rdd_225_5 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 13:59:48 WARN BlockManager: Persisting block rdd_225_5 to disk instead.\n",
      "23/04/03 13:59:48 WARN MemoryStore: Not enough space to cache rdd_225_3 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 13:59:48 WARN BlockManager: Persisting block rdd_225_3 to disk instead.\n",
      "23/04/03 13:59:52 WARN MemoryStore: Not enough space to cache rdd_225_8 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 13:59:52 WARN MemoryStore: Not enough space to cache rdd_225_1 in memory! (computed 97.9 MiB so far)\n",
      "23/04/03 13:59:52 WARN BlockManager: Persisting block rdd_225_1 to disk instead.\n",
      "23/04/03 13:59:52 WARN MemoryStore: Not enough space to cache rdd_225_9 in memory! (computed 3.5 MiB so far)\n",
      "23/04/03 13:59:52 WARN MemoryStore: Not enough space to cache rdd_225_10 in memory! (computed 1560.3 KiB so far)\n",
      "23/04/03 13:59:52 WARN MemoryStore: Not enough space to cache rdd_225_6 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 13:59:53 WARN MemoryStore: Not enough space to cache rdd_225_5 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 13:59:53 WARN MemoryStore: Not enough space to cache rdd_225_3 in memory! (computed 65.2 MiB so far)\n",
      "23/04/03 13:59:53 WARN MemoryStore: Not enough space to cache rdd_225_7 in memory! (computed 65.2 MiB so far)\n",
      "23/04/03 13:59:53 WARN MemoryStore: Not enough space to cache rdd_225_0 in memory! (computed 65.2 MiB so far)\n",
      "23/04/03 13:59:53 WARN MemoryStore: Not enough space to cache rdd_225_1 in memory! (computed 97.9 MiB so far)\n",
      "23/04/03 13:59:54 WARN MemoryStore: Not enough space to cache rdd_225_11 in memory! (computed 147.2 MiB so far)\n",
      "23/04/03 13:59:54 WARN BlockManager: Persisting block rdd_225_11 to disk instead.\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_7 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_8 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_9 in memory! (computed 1560.3 KiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_3 in memory! (computed 3.5 MiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_0 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_1 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_6 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_5 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_4 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 13:59:55 WARN MemoryStore: Not enough space to cache rdd_225_10 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_8 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_6 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_7 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_4 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_3 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_9 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_0 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_10 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_5 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:56 WARN MemoryStore: Not enough space to cache rdd_225_1 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_9 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_3 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_10 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_1 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_8 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_6 in memory! (computed 2.3 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_5 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_7 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_4 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_0 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_5 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_9 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_6 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_7 in memory! (computed 2.3 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_4 in memory! (computed 5.4 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_0 in memory! (computed 2.3 MiB so far)\n",
      "23/04/03 13:59:57 WARN MemoryStore: Not enough space to cache rdd_225_8 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 13:59:58 WARN MemoryStore: Not enough space to cache rdd_225_1 in memory! (computed 12.1 MiB so far)\n",
      "23/04/03 13:59:58 WARN MemoryStore: Not enough space to cache rdd_225_10 in memory! (computed 18.4 MiB so far)\n",
      "23/04/03 13:59:58 WARN MemoryStore: Not enough space to cache rdd_225_3 in memory! (computed 43.5 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=5, labelCol=\"deceased\", seed=42,leafCol=\"leafId\")\n",
    "(train, test) = chiResult.randomSplit([0.8, 0.2])\n",
    "model = rf.fit(train).setFeaturesCol('features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cedb09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/03 14:05:22 WARN MemoryStore: Not enough space to cache rdd_848_0 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:05:22 WARN MemoryStore: Not enough space to cache rdd_848_6 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:07:47 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 132649 ms exceeds timeout 120000 ms\n",
      "23/04/03 14:07:52 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "23/04/03 14:07:53 WARN MemoryStore: Not enough space to cache rdd_848_5 in memory! (computed 43.5 MiB so far)\n",
      "23/04/03 14:07:53 WARN MemoryStore: Not enough space to cache rdd_848_2 in memory! (computed 8.1 MiB so far)\n",
      "23/04/03 14:07:53 WARN BlockManager: Block rdd_848_8 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:54 WARN BlockManager: Persisting block rdd_848_2 to disk instead.\n",
      "23/04/03 14:07:54 WARN MemoryStore: Not enough space to cache rdd_848_4 in memory! (computed 28.6 MiB so far)\n",
      "23/04/03 14:07:54 WARN BlockManager: Persisting block rdd_848_4 to disk instead.\n",
      "23/04/03 14:07:54 WARN BlockManager: Persisting block rdd_848_5 to disk instead.\n",
      "23/04/03 14:07:54 WARN BlockManager: Persisting block rdd_848_6 to disk instead.\n",
      "23/04/03 14:07:54 WARN BlockManager: Persisting block rdd_848_0 to disk instead.\n",
      "23/04/03 14:07:54 ERROR Executor: Exception in task 8.0 in stage 252.0 (TID 960)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n",
      "\tat org.apache.spark.sql.execution.SortExec$$Lambda$3788/1311850748.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2400/1662504732.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$3805/568651151.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "23/04/03 14:07:56 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 252.0 (TID 960),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n",
      "\tat org.apache.spark.sql.execution.SortExec$$Lambda$3788/1311850748.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2400/1662504732.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$3805/568651151.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "23/04/03 14:07:56 WARN TaskSetManager: Lost task 8.0 in stage 252.0 (TID 960) (cs-218992.cs.txstate.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n",
      "\tat org.apache.spark.sql.execution.SortExec$$Lambda$3788/1311850748.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2400/1662504732.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$3805/568651151.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\n",
      "23/04/03 14:07:56 ERROR TaskSetManager: Task 8 in stage 252.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 252:>                                                      (0 + 11) / 12]\r",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_3 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_11 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_11 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_10 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_9 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_1 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_10 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_7 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_2 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 252.0 failed 1 times, most recent failure: Lost task 8.0 in stage 252.0 (TID 960) (cs-218992.cs.txstate.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n",
      "\tat org.apache.spark.sql.execution.SortExec$$Lambda$3788/1311850748.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2400/1662504732.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$3805/568651151.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n",
      "\tat org.apache.spark.sql.execution.SortExec$$Lambda$3788/1311850748.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2400/1662504732.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$3805/568651151.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 1.0 in stage 252.0 (TID 953) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 10.0 in stage 252.0 (TID 962) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 7.0 in stage 252.0 (TID 959) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 2.0 in stage 252.0 (TID 954) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 3.0 in stage 252.0 (TID 955) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 9.0 in stage 252.0 (TID 961) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 11.0 in stage 252.0 (TID 963) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_4 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 4.0 in stage 252.0 (TID 956) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_0 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_5 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Asked to remove block rdd_848_0, which does not exist\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 0.0 in stage 252.0 (TID 952) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 5.0 in stage 252.0 (TID 957) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/03 14:07:57 WARN BlockManager: Putting block rdd_848_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/04/03 14:07:57 WARN BlockManager: Block rdd_848_6 could not be removed as it was not found on disk or in memory\n",
      "23/04/03 14:07:57 WARN BlockManager: Asked to remove block rdd_848_6, which does not exist\n",
      "23/04/03 14:07:57 WARN BlockManager: Asked to remove block rdd_848_5, which does not exist\n",
      "23/04/03 14:07:57 WARN BlockManager: Asked to remove block rdd_848_4, which does not exist\n",
      "23/04/03 14:07:57 WARN TaskSetManager: Lost task 6.0 in stage 252.0 (TID 958) (cs-218992.cs.txstate.edu executor driver): TaskKilled (Stage cancelled)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12481/4099446391.py\", line 4, in <module>\n",
      "    model = rf.fit(train).setFeaturesCol('selectedFeatures')\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/base.py\", line 161, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 335, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 332, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/m_e172/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      3\u001b[0m (train, test) \u001b[38;5;241m=\u001b[39m chiResult\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msetFeaturesCol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselectedFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2068\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2065\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2068\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2070\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/ipykernel/zmqshell.py:541\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    535\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    536\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    538\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    542\u001b[0m }\n\u001b[1;32m    544\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py:281\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py:288\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py:402\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=5, labelCol=\"deceased\", seed=42,leafCol=\"leafId\")\n",
    "(train, test) = chiResult.randomSplit([0.8, 0.2])\n",
    "model = rf.fit(train).setFeaturesCol('selectedFeatures')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f4d90",
   "metadata": {},
   "source": [
    "Deceased & Covid-19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97962369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = assembler.transform(finalDf)\n",
    "df = df.na.drop()\n",
    "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"deceased & covid-19\")\n",
    "chiResult = selector.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fe9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=5, labelCol=\"deceased\", seed=42,leafCol=\"leafId\")\n",
    "(train, test) = chiResult.randomSplit([0.8, 0.2])\n",
    "model = rf.fit(train).setFeaturesCol('features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=5, labelCol=\"deceased\", seed=42,leafCol=\"leafId\")\n",
    "(train, test) = chiResult.randomSplit([0.8, 0.2])\n",
    "model = rf.fit(train).setFeaturesCol('selectedFeatures')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696821f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
